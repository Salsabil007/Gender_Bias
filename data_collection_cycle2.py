from crossref.restful import Works
from crossref.restful import Journals
import requests
import json
import math
import pandas as pd
import numpy as np

##step one //this step runs multiple times. First time, I take 1k unique journals from top_25_single_issn.csv and extract their dois. Second time, I remove the first 1k from head 
##and take the 2nd 1k. 3rd time, I remove the first 2k journals and take the next 1k and so on.
##we didn't run this part as it was alreadt done in cross_ref_doi_json.py


'''def get_data(msg,issn):
    url = "https://api.crossref.org/journals/"+issn+"/works?select=DOI,published&cursor=*&mailto=support@crossref.org"
    res = requests.get(url)
    if res.status_code == 200:
        result = res.json()
        print(result['message']['total-results'])
        cnt = math.ceil(result['message']['total-results']/1000)
        print(cnt)
        
        a = "*"
        for i in range(cnt):
            url = "https://api.crossref.org/journals/"+issn+"/works?select=DOI,published&rows=1000&cursor="+a+"&mailto=support@crossref.org"
            res = requests.get(url)
            if res.status_code != 200:
                print("ohh no")
                return msg
            
            result = res.json()
            
            
            if 'message' in result:
                result['message']['issn'] = str(issn)
                msg.append(result['message'])

                if 'next-cursor' in result['message']:
                    a = result['message']['next-cursor']
                else:
                    return msg
                

            else:
                return msg

    else:
        return msg
    return msg


data = pd.read_csv("top_25_single_issn.csv")
doi = []
#data2 = data.drop_duplicates(subset=['journal'],keep="first")


n = 5500 
data = data.tail(data.shape[0]-n)
#data = data.head(500)

#print(data.tail(5))

issn = data['issn']
print("len of issn ",len(issn))

#exit(0)
#print("issn ",issn)


#remove = pd.read_csv("issn_doi.csv")
#remove = remove['issn'].unique()
#issn2 = [i for i in issn if i not in remove]

#print(len(issn))
#print(len(res))
#print(data['issn'].nunique())
#issn = res
#print(issn2)

count = 0
for i in issn:
    print("issn ",i)
    doi = get_data(doi,i)
    
    print("count ",count)
    count += 1
with open("doi_issn_json_6k2.json", "w") as outfile:
    json.dump(doi, outfile)'''


##step 2: I take the json generated by the step one and extract the doi, year etc from the json file and save it. It is also repeated several times, once for for each 1k journals.
##this time, I am keeping only things that are greater than or equal to 2013.
'''with open('doi_issn_json_6k2.json', 'r') as openfile:
 
    # Reading from json file
    dx1 = json.load(openfile)

doi,issn,yr = [],[],[]

for i in dx1:
    #dx[0]['items'][0]['DOI']
    if 'items' in i:
        #print(i['items'])
        for j in i['items']:
            if 'published' in j:
                if 'date-parts' in j['published']:
                    if j['published']['date-parts'][0][0] < 2013:
                        continue

            if 'DOI' in j:
                doi.append(j['DOI'])
                if 'published' in j:
                    if 'date-parts' in j['published']:
                        yr.append(j['published']['date-parts'][0][0])
                    else:
                        yr.append(0)
                else:
                    yr.append(0)
                if 'issn' in i:
                    issn.append(i['issn'])
                else:
                    issn.append(0)
            else:
                continue
                
dummy = pd.DataFrame()
dummy['doi'] = doi
dummy['issn'] = issn
dummy['yr'] = yr
#dummy = dummy[dummy['yr'] >= 2018]

dummy.to_csv("doi_6k2_from_json_2.csv", index = False)

print(dummy['yr'].min())
print(dummy['yr'].max())
'''



##step 3: I merge the data files generated by the second step.

'''data = pd.read_csv("doi_1k_from_json_2.csv")

tt = pd.read_csv("doi_2k_from_json_2.csv")
data = pd.concat([data, tt], ignore_index=True)

tt = pd.read_csv("doi_3k_from_json_2.csv")
data = pd.concat([data, tt], ignore_index=True)

tt = pd.read_csv("doi_4k_from_json_2.csv")
data = pd.concat([data, tt], ignore_index=True)

tt = pd.read_csv("doi_5k_from_json_2.csv")
data = pd.concat([data, tt], ignore_index=True)

tt = pd.read_csv("doi_6k1_from_json_2.csv")
data = pd.concat([data, tt], ignore_index=True)

tt = pd.read_csv("doi_6k2_from_json_2.csv")
data = pd.concat([data, tt], ignore_index=True)

data['issn'] = data['issn'].astype(str)
print(data.dtypes)

print(len(data))
print(data['issn'].nunique())
print(data['doi'].nunique())

data2 = pd.read_csv("top_25_issn.csv")
data['issn'] = data['issn'].astype(str)

data2['issn'] = data2['issn'].str.lstrip('0')
data['issn'] = data['issn'].str.lstrip('0')
new = data.merge(data2, on = ['issn'],how='inner')

print(len(new))

new.to_csv("total_doi_crossref_cycle2.csv", index = False)'''


#step 4: merging dois with altmetric data dump (data_dump2.py)
'''data = pd.read_csv("interim.csv")
#print(len(data))
#print(data['doi'].nunique())
data = data.drop_duplicates(subset=['doi'],keep="first")
print("len of interim ",len(data))
print("len of unique doi ", data['doi'].nunique())


new = pd.read_csv("total_doi_crossref_cycle2.csv")
new = new.drop_duplicates()
print("len of crossref ",len(new))
print("unique doi in crossref ", new['doi'].nunique())
#new = new[new['yr'] < 2023]
#print("len of crossref doi before 2023",len(new))
#print("unique doi in cross ref ",new['doi'].nunique())

joined = new.merge(data,on=['doi'],how='left')
print("length of the merged dataframe ",len(joined))
print("unique doi in merged ", joined['doi'].nunique())


print(joined.head(10))
#####new added
joined.loc[joined['news_cnt'].isna(), 'news_cnt'] = 0

print("%%%%%%%%%%%%%%%%%%%%%")
print(joined.head(10))

final_list = pd.DataFrame()
final_list['doi'] = joined['doi']
final_list['news_cnt'] = joined['news_cnt']
print("len of final_list ",len(final_list))
final_list['news_cnt'] = final_list['news_cnt'].astype(int)

#print(final_list['news_cnt'].isna().sum())
#print(final_list['doi'].isna().sum())


final_list.to_csv("doi_news_final_cycle2.csv", index = False)'''
new_final_list = pd.read_csv("doi_news_final_cycle2.csv")
print(new_final_list.dtypes)
print(len(new_final_list))
print("TTT")
print(len(new_final_list[new_final_list['news_cnt'] == 0])+len(new_final_list[new_final_list['news_cnt'] > 0]))